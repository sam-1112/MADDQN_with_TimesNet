import os
import numpy as np
import glob
import torch
from tqdm import tqdm
import matplotlib.pyplot as plt
from data.data import FetchData
from utils.dataPreprocess import PreprocessData
from environment.maddqnEnv import MADDQNENV
from agents.subAgent import subAgent
from agents.finalAgent import finalAgent
import json
from datetime import datetime

class MADDQN:
    def __init__(self, configs, args):
        self.configs = configs
        self.args = args
        # æ­£ç¢ºè¨­å®šè¨­å‚™
        self.device = torch.device("cuda" if torch.cuda.is_available() and configs['gpu']['use_gpu'] else "cpu")
        print(f"Using device: {self.device}")
        
        self.returnAgentModel = subAgent(agentType='return', configs=self.configs)
        self.riskAgentModel = subAgent(agentType='risk', configs=self.configs)
        self.finalAgentModel = finalAgent(configs=self.configs)

        self.reward_stats = {
            'risk_agent': {'rewards': []},
            'return_agent': {'rewards': []},
            'final_agent': {'rewards': []}
        }
        
        # æ·»åŠ  episode å±¤ç´šçš„å›å ±è¿½è¹¤
        self.episode_returns = {
            'risk_agent': [],
            'return_agent': [],
            'final_agent': []
        }
        
        self.subEnvList = []

        self.test_dates = {
            'train_start_date': self.configs['data']['train_start_date'],
            'train_end_date': self.configs['data']['train_end_date'],
            'test_start_date': self.configs['data']['test_start_date'],
            'test_end_date': self.configs['data']['test_end_date']
        }

        self.test_prices = {
            'train_prices': [],
            'test_prices': []
        }


    def initialize(self):
        """
        åˆå§‹åŒ–MADDQNç³»çµ±çš„æ‰€æœ‰çµ„ä»¶
        """
        print("æ­£åœ¨åˆå§‹åŒ–MADDQNç³»çµ±...")
        
        # åˆå§‹åŒ–è³‡æ–™è™•ç†å™¨å’Œç’°å¢ƒ
        self.fetcher = FetchData(ticker=self.configs['data']['ticker'], start_date=self.configs['data']['train_start_date'], end_date=self.configs['data']['test_end_date'])
        self.unprocessed_data = self.fetcher.get_data()
        dates = [self.configs['data']['train_start_date'], self.configs['data']['train_end_date'], self.configs['data']['test_start_date'], self.configs['data']['test_end_date']]
        self.data_processor = PreprocessData(data=self.unprocessed_data, window_size=self.configs['env']['window_size'], dates=dates)
        self.train_data, self.test_data = self.data_processor.timeSeriesData()
        
        window_size = self.configs['env']['window_size']
    
        # è¨ˆç®—å¯¦éš›å¯ç”¨çš„æ•¸æ“šç´¢å¼•ç¯„åœ
        total_sequences = len(self.train_data) + len(self.test_data)
        available_data_start_idx = window_size - 1  # ç¬¬ä¸€å€‹å¯ç”¨åºåˆ—å°æ‡‰çš„åŸå§‹æ•¸æ“šç´¢å¼•
        available_data_end_idx = available_data_start_idx + total_sequences - 1
        
        # å¯¦éš›çš„è¨“ç·´é›†æ—¥æœŸç¯„åœ
        self.actual_train_start_date = self.unprocessed_data.index[available_data_start_idx]
        train_end_idx = available_data_start_idx + len(self.train_data) - 1
        self.actual_train_end_date = self.unprocessed_data.index[train_end_idx]
        
        # å¯¦éš›çš„æ¸¬è©¦é›†æ—¥æœŸç¯„åœ  
        test_start_idx = train_end_idx + 1
        self.actual_test_start_date = self.unprocessed_data.index[test_start_idx]
        self.actual_test_end_date = self.unprocessed_data.index[available_data_end_idx]
        
        # æ‰“å°å¯¦éš›æ—¥æœŸç¯„åœèˆ‡é…ç½®æ¯”è¼ƒ
        print(f"\nğŸ“… æ—¥æœŸç¯„åœæ¯”è¼ƒ:")
        print(f"é…ç½®çš„è¨“ç·´æœŸé–“: {self.configs['data']['train_start_date']} ~ {self.configs['data']['train_end_date']}")
        print(f"å¯¦éš›çš„è¨“ç·´æœŸé–“: {self.actual_train_start_date.date()} ~ {self.actual_train_end_date.date()}")
        print(f"é…ç½®çš„æ¸¬è©¦æœŸé–“: {self.configs['data']['test_start_date']} ~ {self.configs['data']['test_end_date']}")
        print(f"å¯¦éš›çš„æ¸¬è©¦æœŸé–“: {self.actual_test_start_date.date()} ~ {self.actual_test_end_date.date()}")
        print(f"çª—å£å¤§å°å½±éŸ¿: å‰ {window_size-1} å¤©çš„æ•¸æ“šç”¨æ–¼æ§‹å»ºçª—å£ï¼Œç„¡æ³•ä½œç‚ºç¨ç«‹æ¨£æœ¬")
    
        
        # Initialize all subagents' environments - ä¿®æ­£åƒæ•¸
        self.subEnvList = []
        for agentType in ['risk', 'return']:
            subEnv = MADDQNENV(
                configs=self.configs,
                tradeData=self.unprocessed_data,  # ç§»é™¤ configs åƒæ•¸
                window_size=self.configs['env']['window_size'],
                n_agents=self.configs['env'][f'{agentType}_agent']
            )
            self.subEnvList.append(subEnv)

        # Initialize the main environment - ä¿®æ­£åƒæ•¸
        self.env = MADDQNENV(
            configs=self.configs,
            tradeData=self.unprocessed_data,  # ç§»é™¤ configs åƒæ•¸
            window_size=self.configs['env']['window_size'],
            n_agents=self.configs['env']['risk_agent'] + self.configs['env']['return_agent']
        )
        
    def training_process(self):
        """
        åŸ·è¡Œ MADDQN è¨“ç·´éç¨‹ - æ ¹æ“šè«–æ–‡æ¼”ç®—æ³•å¯¦ç¾
        Algorithm: Multi-Agent Deep Double Q-Learning with TimesNet
        """
        print("é–‹å§‹ MADDQN è¨“ç·´éç¨‹...")
        self.initialize()
        
        # ç²å–è¨“ç·´æ•¸æ“šç¸½é•·åº¦
        total_sequences = len(self.train_data)
        print(f"Total training sequences: {total_sequences}")

        # ä¸»è¨“ç·´å¾ªç’° - for episode = 1 to M do
        for episode in range(1, self.configs['training']['episodes'] + 1):
            print(f"\n=== Episode {episode}/{self.configs['training']['episodes']} ===")
            
            # é‡ç½®ç’°å¢ƒ - Initialize sequence s1 = {x1} and preprocessed state Ï†1 = Ï†(s1)
            self.env.reset()
            for subEnv in self.subEnvList:
                subEnv.reset()
            
            # åˆå§‹åŒ–episodeè®Šé‡
            episode_risk_return = 0.0
            episode_return_return = 0.0
            episode_final_return = 0.0
            current_episode_losses = {
                'risk_agent': [],
                'return_agent': [],
                'final_agent': []
            }
            
            # å­˜å„²æ‰€æœ‰å­ä»£ç†çš„Qå€¼ - Q-values of each agent needs to be stored
            num_agents = self.configs['env']['risk_agent'] + self.configs['env']['return_agent']
            max_timesteps = min(total_sequences, self.configs.get('training', {}).get('max_episode_steps', total_sequences))
            QValues_List = np.zeros((num_agents, max_timesteps, 3), dtype=np.float32)
            
            # æ™‚é–“æ­¥å¾ªç’° - ä½¿ç”¨åµŒå¥—é€²åº¦æ¢
            timestep_pbar = tqdm(
                range(max_timesteps),
                desc=f"Ep{episode:02d} Steps",
                unit="step",
                leave=False,  # ä¸ä¿ç•™æ™‚é–“æ­¥é€²åº¦æ¢
                ncols=140,
                bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'
            )

            # æ™‚é–“æ­¥å¾ªç’° - for i=1 to T do
            for timestep in timestep_pbar:
                if timestep >= total_sequences:
                    break
                    
                # print(f"  Timestep {timestep + 1}/{max_timesteps}")
                
                # ç²å–ç•¶å‰åºåˆ— si = {xi} and preprocessed state Ï†i = Ï†(si)
                current_sequence = self.train_data[timestep]
                preprocessed_state = torch.tensor(current_sequence, dtype=torch.float32).to(self.device)
                
                # æ›´æ–°ç’°å¢ƒç‹€æ…‹
                self.env.state = current_sequence
                old_state = current_sequence.copy()
                agent_rewards = []  # ç”¨æ–¼å­˜å„²æ¯å€‹ä»£ç†çš„çå‹µ
                
                # å­ä»£ç†è™•ç†å¾ªç’° - for j=1 to J do
                for agentIndex in range(num_agents):
                    # ç¢ºå®šä»£ç†é¡å‹å’Œæ¨¡å‹
                    if agentIndex < self.configs['env']['risk_agent']:
                        model = self.riskAgentModel
                        agent = 'risk'
                        subAgentEnv = self.subEnvList[0]
                    else:
                        model = self.returnAgentModel
                        agent = 'return'
                        subAgentEnv = self.subEnvList[1]
                    
                    # Select aj,i with Îµ-greedy method
                    # Otherwise, calculate Qj = Q(Ï†(si), aj; Î¸j)
                    # Select aj,i = argmaxaj Qj
                    action, QValues = model.act(preprocessed_state)
                    QValues_List[agentIndex, timestep, :] = QValues
                    
                    # Sub-agent executes action aj,i in the environment
                    # Sub-agent gets reward rj,i and observes next state si+1, Ï†i+1 = Ï†(si+1)
                    next_state, reward, done, info = subAgentEnv.step(
                        action, agentType=agent, tradeAmount=self.configs['env']['trade_amount']
                    )

                    agent_rewards.append(reward)  # æ”¶é›†æ¯å€‹ä»£ç†çš„çå‹µ
                    
                    # ç´¯ç©çå‹µ
                    if agent == 'risk':
                        episode_risk_return += reward
                        # è¨˜éŒ„å–®æ­¥çå‹µç”¨æ–¼åˆ†ä½ˆåˆ†æ
                        self.reward_stats['risk_agent']['rewards'].append(reward)
                    elif agent == 'return':
                        episode_return_return += reward
                        # è¨˜éŒ„å–®æ­¥çå‹µç”¨æ–¼åˆ†ä½ˆåˆ†æ
                        self.reward_stats['return_agent']['rewards'].append(reward)
                    
                    # æª¢æŸ¥çµ‚æ­¢æ¢ä»¶
                    is_last_timestep = (timestep == max_timesteps - 1)
                    episode_done = done or is_last_timestep
                    
                    # Store the transition (Ï†i, aj,i, rj,i, Ï†i+1) in Bj
                    model.replay_buffer.add({
                        'agent': agent,
                        'state': old_state,
                        'action': action,
                        'reward': reward,
                        'next_state': next_state,
                        'done': episode_done
                    })
                    
                    # å­ä»£ç†å­¸ç¿’ - for j=1 to J for each sub-agent j do
                    self._train_subagent_step(model, agent, current_episode_losses, timestep)
                
                # Final Agentè™•ç†
                # Select aF,i with Îµ-greedy method
                # Otherwise, select aF,i = argmaxaF Q(Ï†(si), Q1, Q2, â‹¯, QJ, aF; Î¸F)
                current_subagent_qvalues = QValues_List[:, timestep, :]
                final_action, final_q_values = self.finalAgentModel.act(
                    current_sequence, current_subagent_qvalues
                )
                
                # Final agent executes action aF,i in the environment
                # Final agent gets reward ri and observes next state si+1, Ï†i+1 = Ï†(si+1)
                final_next_state, final_reward, final_done, info = self.env.step(
                    final_action, agentType='final', tradeAmount=self.configs['env']['trade_amount']
                )
                
                episode_final_return += final_reward
                # è¨˜éŒ„final agentçš„å–®æ­¥çå‹µ
                self.reward_stats['final_agent']['rewards'].append(final_reward)
                final_episode_done = final_done or is_last_timestep
                
                # æº–å‚™ä¸‹ä¸€å€‹timestepçš„å­ä»£ç†Qå€¼
                next_timestep = min(timestep + 1, max_timesteps - 1)
                next_subagent_qvalues = QValues_List[:, next_timestep, :] if not final_episode_done else current_subagent_qvalues
                
                # Store the transition (Ï†i, aF,i, rF,i, Q1, Q2, â‹¯, QJ, Ï†i+1) in BF
                self.finalAgentModel.replay_buffer.add({
                    'agent': 'final',
                    'state': old_state,
                    'action': final_action,
                    'reward': final_reward,
                    'subagent_qvalues': current_subagent_qvalues.copy(),
                    'next_state': final_next_state,
                    'next_subagent_qvalues': next_subagent_qvalues.copy(),
                    'done': final_episode_done
                })
                
                # Final Agentå­¸ç¿’
                self._train_final_agent_step(current_episode_losses, timestep)
                
                # ç›®æ¨™ç¶²çµ¡æ›´æ–° - Update the parameters of the target network every C steps
                if (timestep + 1) % self.configs['training']['target_update_frequency'] == 0:
                    self._update_target_networks(timestep + 1)

                # æ›´æ–°æ™‚é–“æ­¥é€²åº¦æ¢çš„å¾Œç¶´ä¿¡æ¯
                avg_reward = np.mean(agent_rewards) if agent_rewards else 0
                timestep_pbar.set_postfix({
                    'Risk_R': f'{episode_risk_return:.1f}',
                    'Return_R': f'{episode_return_return:.1f}',
                    'Final_R': f'{episode_final_return:.1f}'
                })

            # é—œé–‰æ™‚é–“æ­¥é€²åº¦æ¢
            timestep_pbar.close()

            # EpisodeçµæŸå¾Œçš„è™•ç†
            # Epsilonè¡°æ¸›
            self.returnAgentModel.decay_epsilon(episode=episode)
            self.riskAgentModel.decay_epsilon(episode=episode)
            self.finalAgentModel.decay_epsilon(episode=episode)

            # è¨˜éŒ„episodeçµæœ
            self._record_episode_results(episode, episode_risk_return, episode_return_return, 
                                        episode_final_return, current_episode_losses)

            
            # æ¯10å€‹episodeæ‰“å°é€²åº¦
            if episode % 10 == 0:
                self._print_training_progress(episode)
                # åˆ†æçå‹µåˆ†ä½ˆ
                self.analyze_reward_distribution()
            
            # save models for all agents
            self.save_models(episode)

        # è¨“ç·´å®Œæˆ
        print("\nMADDQN è¨“ç·´éç¨‹å®Œæˆï¼")
        
        # æœ€çµ‚çå‹µåˆ†ä½ˆåˆ†æ
        self.analyze_reward_distribution()
        self.plot_reward_distribution()
        self.plot_episode_returns()
        self.plot_loss_trends()
        self.plot_stock_history()
    
    def _train_subagent_step(self, model, agent, current_episode_losses, timestep):
        """
        è¨“ç·´å­ä»£ç†çš„å–®æ­¥ - ç›´æ¥å¯¦ç¾DQNå­¸ç¿’é‚è¼¯
        
        Args:
            model: å­ä»£ç†æ¨¡å‹ (subAgent)
            agent: ä»£ç†é¡å‹ ('risk' æˆ– 'return')
            current_episode_losses: ç•¶å‰episodeçš„æå¤±è¨˜éŒ„å­—å…¸
            timestep: ç•¶å‰æ™‚é–“æ­¥
        """
        try:
            # æª¢æŸ¥replay bufferæ˜¯å¦æœ‰è¶³å¤ çš„æ¨£æœ¬é€²è¡Œè¨“ç·´
            if model.replay_buffer.__len__() < self.configs['training']['batch_size']:
                # print(f"    {agent.capitalize()} Agent: Insufficient samples in replay buffer (need {self.configs['training']['batch_size']}, have {model.replay_buffer.__len__()})")
                return
            
            # å¾replay bufferä¸­æ¡æ¨£ä¸€å€‹batch
            minibatch = model.replay_buffer.sample(self.configs['training']['batch_size'])
            
            # æå–batchæ•¸æ“š
            states = []
            actions = []
            rewards = []
            next_states = []
            dones = []
            
            for exp in minibatch:
                states.append(exp['state'])
                actions.append(exp['action'])
                rewards.append(exp['reward'])
                next_states.append(exp['next_state'])
                dones.append(exp['done'])
            
            # è½‰æ›ç‚ºå¼µé‡
            states = torch.tensor(np.array(states), dtype=torch.float32).to(model.device)
            next_states = torch.tensor(np.array(next_states), dtype=torch.float32).to(model.device)
            rewards = torch.tensor(rewards, dtype=torch.float32).to(model.device)
            dones = torch.tensor(dones, dtype=torch.bool).to(model.device)
            
            # è™•ç†å‹•ä½œï¼šç’°å¢ƒå‹•ä½œ [-1, 0, 1] -> æ¨¡å‹ç´¢å¼• [0, 1, 2]
            action_mapping = {-1: 0, 0: 1, 1: 2}
            mapped_actions = [action_mapping.get(action, 1) for action in actions]  # é»˜èªç‚º1(hold)
            actions = torch.tensor(mapped_actions, dtype=torch.long).to(model.device)
            
            # ç¢ºä¿ç‹€æ…‹ç¶­åº¦æ­£ç¢º (batch_size, seq_len, features)
            if states.dim() == 2:
                states = states.unsqueeze(1)  # æ·»åŠ åºåˆ—ç¶­åº¦
            if next_states.dim() == 2:
                next_states = next_states.unsqueeze(1)
            
            # ç•¶å‰Qå€¼
            current_q_values = model.policy_network(states)  # [batch_size, 3]
            current_q_values_selected = current_q_values.gather(1, actions.unsqueeze(1)).squeeze(1)
            
            # è¨ˆç®—ç›®æ¨™Qå€¼
            with torch.no_grad():
                # Double DQN: ä½¿ç”¨policy networké¸æ“‡å‹•ä½œï¼Œtarget networkè©•ä¼°Qå€¼
                next_q_values_policy = model.policy_network(next_states)
                next_actions = next_q_values_policy.argmax(1)
                
                next_q_values_target = model.target_network(next_states)
                next_q_values_selected = next_q_values_target.gather(1, next_actions.unsqueeze(1)).squeeze(1)
                
                # è¨ˆç®—ç›®æ¨™å€¼ï¼šr + Î³ * max_a Q(s', a) * (1 - done)
                target_q_values = rewards + (model.gamma * next_q_values_selected * ~dones)
            
            # è¨ˆç®—æå¤±
            loss = torch.nn.functional.mse_loss(current_q_values_selected, target_q_values)
            
            # æª¢æŸ¥æå¤±æ˜¯å¦æœ‰æ•ˆ
            if torch.isnan(loss) or torch.isinf(loss):
                print(f"    {agent.capitalize()} Agent: Invalid loss detected (NaN/Inf), skipping update")
                return
            
            # åå‘å‚³æ’­
            model.optimizer.zero_grad()
            loss.backward()
            model.optimizer.step()
            
            # è¨˜éŒ„æå¤±
            loss_value = loss.item()
            agent_key = f"{agent}_agent"
            current_episode_losses[agent_key].append(loss_value)
            # print(f"    {agent.capitalize()} Agent Loss at Timestep {timestep + 1}: {loss_value:.6f}")
            
        except Exception as e:
            print(f"    Error in {agent} agent training at timestep {timestep + 1}: {e}")
            import traceback
            traceback.print_exc()
    
    def _train_final_agent_step(self, current_episode_losses, timestep):
        """
        è¨“ç·´æœ€çµ‚ä»£ç†çš„å–®æ­¥ - ç›´æ¥å¯¦ç¾DQNå­¸ç¿’é‚è¼¯
        
        Args:
            current_episode_losses: ç•¶å‰episodeçš„æå¤±è¨˜éŒ„å­—å…¸
            timestep: ç•¶å‰æ™‚é–“æ­¥
        """
        try:
            # æª¢æŸ¥replay bufferæ˜¯å¦æœ‰è¶³å¤ çš„æ¨£æœ¬é€²è¡Œè¨“ç·´
            if self.finalAgentModel.replay_buffer.__len__() < self.configs['training']['batch_size']:
                # print(f"    Final Agent: Insufficient samples in replay buffer (need {self.configs['training']['batch_size']}, have {self.finalAgentModel.replay_buffer.__len__()})")
                return
            
            # å¾replay bufferä¸­æ¡æ¨£ä¸€å€‹batch
            minibatch = self.finalAgentModel.replay_buffer.sample(self.configs['training']['batch_size'])
            
            # æå–batchæ•¸æ“š
            market_states = []
            subagent_qvalues = []
            actions = []
            rewards = []
            next_market_states = []
            next_subagent_qvalues = []
            dones = []
            
            for exp in minibatch:
                market_states.append(exp['state'])
                subagent_qvalues.append(exp['subagent_qvalues'])
                actions.append(exp['action'])
                rewards.append(exp['reward'])
                next_market_states.append(exp['next_state'])
                next_subagent_qvalues.append(exp['next_subagent_qvalues'])
                dones.append(exp['done'])
            
            # è½‰æ›ç‚ºå¼µé‡
            market_states = torch.tensor(np.array(market_states), dtype=torch.float32).to(self.finalAgentModel.device)
            subagent_qvalues = torch.tensor(np.array(subagent_qvalues), dtype=torch.float32).to(self.finalAgentModel.device)
            next_market_states = torch.tensor(np.array(next_market_states), dtype=torch.float32).to(self.finalAgentModel.device)
            next_subagent_qvalues = torch.tensor(np.array(next_subagent_qvalues), dtype=torch.float32).to(self.finalAgentModel.device)
            rewards = torch.tensor(rewards, dtype=torch.float32).to(self.finalAgentModel.device)
            dones = torch.tensor(dones, dtype=torch.bool).to(self.finalAgentModel.device)
            
            # è™•ç†å‹•ä½œï¼šç’°å¢ƒå‹•ä½œ [-1, 0, 1] -> æ¨¡å‹ç´¢å¼• [0, 1, 2]
            action_mapping = {-1: 0, 0: 1, 1: 2}
            mapped_actions = [action_mapping.get(action, 1) for action in actions]  # é»˜èªç‚º1(hold)
            actions = torch.tensor(mapped_actions, dtype=torch.long).to(self.finalAgentModel.device)
            
            # ç¢ºä¿ç‹€æ…‹ç¶­åº¦æ­£ç¢º
            if market_states.dim() == 2:
                market_states = market_states.unsqueeze(1)  # æ·»åŠ åºåˆ—ç¶­åº¦
            if next_market_states.dim() == 2:
                next_market_states = next_market_states.unsqueeze(1)
            
            # ç¢ºä¿å­ä»£ç†Qå€¼ç¶­åº¦æ­£ç¢º
            if subagent_qvalues.dim() == 2:
                # (batch_size, n_agents*3) -> (batch_size, n_agents, 3)
                n_agents = self.configs['env']['risk_agent'] + self.configs['env']['return_agent']
                subagent_qvalues = subagent_qvalues.reshape(-1, n_agents, 3)
            if next_subagent_qvalues.dim() == 2:
                n_agents = self.configs['env']['risk_agent'] + self.configs['env']['return_agent']
                next_subagent_qvalues = next_subagent_qvalues.reshape(-1, n_agents, 3)
            
            # å°‡å­ä»£ç†Qå€¼å±•å¹³ç‚ºè¼¸å…¥ç‰¹å¾µ
            batch_size = subagent_qvalues.size(0)
            subagent_qvalues_flat = subagent_qvalues.view(batch_size, -1)  # [batch_size, n_agents*3]
            next_subagent_qvalues_flat = next_subagent_qvalues.view(batch_size, -1)
            
            # ç•¶å‰Qå€¼ - MSCNNéœ€è¦å¸‚å ´æ•¸æ“šå’Œå­ä»£ç†Qå€¼ä½œç‚ºè¼¸å…¥
            current_q_values = self.finalAgentModel.policy_network(market_states, subagent_qvalues_flat)  # [batch_size, 3]
            current_q_values_selected = current_q_values.gather(1, actions.unsqueeze(1)).squeeze(1)
            
            # è¨ˆç®—ç›®æ¨™Qå€¼
            with torch.no_grad():
                # Double DQN: ä½¿ç”¨policy networké¸æ“‡å‹•ä½œï¼Œtarget networkè©•ä¼°Qå€¼
                next_q_values_policy = self.finalAgentModel.policy_network(next_market_states, next_subagent_qvalues_flat)
                next_actions = next_q_values_policy.argmax(1)
                
                next_q_values_target = self.finalAgentModel.target_network(next_market_states, next_subagent_qvalues_flat)
                next_q_values_selected = next_q_values_target.gather(1, next_actions.unsqueeze(1)).squeeze(1)
                
                # è¨ˆç®—ç›®æ¨™å€¼ï¼šr + Î³ * max_a Q(s', a) * (1 - done)
                target_q_values = rewards + (self.finalAgentModel.gamma * next_q_values_selected * ~dones)
            
            # è¨ˆç®—æå¤±
            loss = torch.nn.functional.mse_loss(current_q_values_selected, target_q_values)
            
            # æª¢æŸ¥æå¤±æ˜¯å¦æœ‰æ•ˆ
            if torch.isnan(loss) or torch.isinf(loss):
                print(f"    Final Agent: Invalid loss detected (NaN/Inf), skipping update")
                return
            
            # åå‘å‚³æ’­
            self.finalAgentModel.optimizer.zero_grad()
            loss.backward()
            self.finalAgentModel.optimizer.step()
            
            # è¨˜éŒ„æå¤±
            loss_value = loss.item()
            current_episode_losses['final_agent'].append(loss_value)
            # print(f"    Final Agent Loss at Timestep {timestep + 1}: {loss_value:.6f}")
            
        except Exception as e:
            print(f"    Error in final agent training at timestep {timestep + 1}: {e}")
            import traceback
            traceback.print_exc()
    
    def _update_target_networks(self, timestep):
        """
        æ›´æ–°æ‰€æœ‰å­ä»£ç†å’Œæœ€çµ‚ä»£ç†çš„ç›®æ¨™ç¶²çµ¡
        """
        self.returnAgentModel.target_network.load_state_dict(self.returnAgentModel.policy_network.state_dict())
        self.riskAgentModel.target_network.load_state_dict(self.riskAgentModel.policy_network.state_dict())
        self.finalAgentModel.target_network.load_state_dict(self.finalAgentModel.policy_network.state_dict())
        # print(f"  Target networks updated at Timestep {timestep + 1}")
    

    def _record_episode_results(self, episode, risk_return, return_return, final_return, losses):
        """
        è¨˜éŒ„æ¯å€‹episodeçš„æ¯å€‹ä»£ç†çš„å›å ±ç‡å’Œæå¤±å€¼
        """
        # è¨˜éŒ„ episode å±¤ç´šçš„å›å ± - é€™æ˜¯ä¿®æ­£çš„é‡é»
        self.episode_returns['risk_agent'].append(risk_return)
        self.episode_returns['return_agent'].append(return_return)
        self.episode_returns['final_agent'].append(final_return)
        print(f"Episode {episode} Results:")
        print(f"  Risk Agent Return: {risk_return:.2f}, Losses: {np.mean(losses['risk_agent']):.4f}")
        print(f"  Return Agent Return: {return_return:.2f}, Losses: {np.mean(losses['return_agent']):.4f}")
        print(f"  Final Agent Return: {final_return:.2f}, Losses: {np.mean(losses['final_agent']):.4f}")
    
    def _print_training_progress(self, episode):
        """
        æ‰“å°è¨“ç·´é€²åº¦
        """
        print(f"\n=== Training Progress after Episode {episode} ===")
        print(f"  Risk Agent Epsilon: {self.riskAgentModel.epsilon:.3f}")
        print(f"  Return Agent Epsilon: {self.returnAgentModel.epsilon:.3f}")
        print(f"  Final Agent Epsilon: {self.finalAgentModel.epsilon:.3f}")
    
    def analyze_reward_distribution(self):
        """
        åˆ†ææ¯å€‹ä»£ç†çš„çå‹µåˆ†ä½ˆ
        """
        print("\n=== Reward Distribution Analysis ===")
        for agent, stats in self.reward_stats.items():
            rewards = stats['rewards']
            if rewards:
                mean_reward = np.mean(rewards)
                std_reward = np.std(rewards)
                print(f"  {agent.capitalize()} Agent - Mean Reward: {mean_reward:.2f}, Std Dev: {std_reward:.2f}")
            else:
                print(f"  {agent.capitalize()} Agent - No rewards recorded.")
        print("=== End of Reward Distribution Analysis ===\n")
    
    def plot_reward_distribution(self):
        """
        ç¹ªè£½æ¯å€‹ä»£ç†çš„çå‹µåˆ†ä½ˆåœ–
        """
        
        plt.figure(figsize=(12, 6))
        for agent, stats in self.reward_stats.items():
            rewards = stats['rewards']
            if rewards:
                plt.hist(rewards, bins=50, alpha=0.5, label=f"{agent.capitalize()} Agent")
        
        plt.title("Reward Distribution of Agents")
        plt.xlabel("Reward")
        plt.ylabel("Frequency")
        plt.legend()
        plt.grid()
        plt.savefig("reward_distribution.png")

    def plot_episode_returns(self):
        """
        ç¹ªè£½æ¯å€‹episodeçš„å›å ±ç‡è¶¨å‹¢åœ–
        """
        episodes = list(range(1, self.configs['training']['episodes'] + 1))
        risk_returns = self.episode_returns['risk_agent']
        return_returns = self.episode_returns['return_agent'] 
        final_returns = self.episode_returns['final_agent']
        
        # å¦‚æœepisodeæ•¸é‡ä¸åŒ¹é…ï¼Œé€²è¡Œèª¿æ•´
        expected_episodes = self.configs['training']['episodes']
        if len(risk_returns) < expected_episodes:
            # å¦‚æœæ•¸æ“šä¸è¶³ï¼Œç”¨0å¡«å……
            risk_returns.extend([0] * (expected_episodes - len(risk_returns)))
            return_returns.extend([0] * (expected_episodes - len(return_returns)))
            final_returns.extend([0] * (expected_episodes - len(final_returns)))
        elif len(risk_returns) > expected_episodes:
            # å¦‚æœæ•¸æ“šéå¤šï¼Œæˆªå–
            risk_returns = risk_returns[:expected_episodes]
            return_returns = return_returns[:expected_episodes]
            final_returns = final_returns[:expected_episodes]

        plt.figure(figsize=(12, 6))
        plt.plot(episodes, risk_returns, label='Risk Agent Returns', marker='o', alpha=0.7)
        plt.plot(episodes, return_returns, label='Return Agent Returns', marker='o', alpha=0.7)
        plt.plot(episodes, final_returns, label='Final Agent Returns', marker='o', alpha=0.7)
        
        plt.title("Episode Returns of Agents")
        plt.xlabel("Episode")
        plt.ylabel("Cumulative Return per Episode")
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.savefig("episode_returns.png", dpi=300, bbox_inches='tight')
        plt.show()
        print("ğŸ“Š Episodeå›å ±è¶¨å‹¢åœ–å·²ä¿å­˜ç‚º 'episode_returns.png'")
    
    def plot_loss_trends(self):
        """
        ç¹ªè£½æ¯å€‹ä»£ç†çš„æå¤±è¶¨å‹¢åœ–
        """
        episodes = list(range(1, self.configs['training']['episodes'] + 1))
        
        risk_losses = [np.mean(self.reward_stats['risk_agent']['rewards'][i::self.configs['training']['episodes']]) for i in range(self.configs['training']['episodes'])]
        return_losses = [np.mean(self.reward_stats['return_agent']['rewards'][i::self.configs['training']['episodes']]) for i in range(self.configs['training']['episodes'])]
        final_losses = [np.mean(self.reward_stats['final_agent']['rewards'][i::self.configs['training']['episodes']]) for i in range(self.configs['training']['episodes'])]

        plt.figure(figsize=(12, 6))
        plt.plot(episodes, risk_losses, label='Risk Agent Losses', marker='o')
        plt.plot(episodes, return_losses, label='Return Agent Losses', marker='o')
        plt.plot(episodes, final_losses, label='Final Agent Losses', marker='o')
        
        plt.title("Loss Trends of Agents")
        plt.xlabel("Episode")
        plt.ylabel("Average Loss")
        plt.legend()
        plt.grid()
        plt.savefig("loss_trends.png")

    def plot_stock_history(self):
        """
        ç¹ªè£½è‚¡ç¥¨æ­·å²åƒ¹æ ¼åœ–
        """
        
        try:
            # æª¢æŸ¥æ•¸æ“šçµæ§‹ä¸¦æ­£ç¢ºç²å–æ—¥æœŸå’Œåƒ¹æ ¼
            dates = self.unprocessed_data.index  # å·²ç¢ºèªindexæ˜¯DatetimeIndex
            
            # è™•ç†MultiIndexåˆ—çµæ§‹ï¼š('Close', 'DJI')
            if ('Close', 'DJI') in self.unprocessed_data.columns:
                prices = self.unprocessed_data[('Close', 'DJI')]
            elif 'Close' in [col[0] if isinstance(col, tuple) else col for col in self.unprocessed_data.columns]:
                # æ‰¾åˆ°Closeç›¸é—œçš„åˆ—
                close_col = None
                for col in self.unprocessed_data.columns:
                    if isinstance(col, tuple) and col[0] == 'Close':
                        close_col = col
                        break
                    elif col == 'Close':
                        close_col = col
                        break
                if close_col:
                    prices = self.unprocessed_data[close_col]
                    print(f"Using column '{close_col}' as close price")
                else:
                    print("Error: No close price column found")
                    return
            else:
                print("Error: No close price column found")
                print(f"Available columns: {list(self.unprocessed_data.columns)}")
                return
            
            # ç¢ºä¿åƒ¹æ ¼æ˜¯æ•¸å€¼é¡å‹ä¸¦è½‰æ›ç‚ºç´”æ•¸å€¼
            prices = prices.values  # è½‰æ›ç‚ºnumpyæ•¸çµ„
            prices = np.array(prices, dtype=float)  # ç¢ºä¿æ˜¯floaté¡å‹
            
            # ç²å–ç´”æ•¸å€¼è€Œä¸æ˜¯æ•¸çµ„
            start_price = float(prices[0]) if len(prices) > 0 else 0.0
            end_price = float(prices[-1]) if len(prices) > 0 else 0.0
            total_return = ((end_price - start_price) / start_price * 100) if start_price != 0 else 0.0
            
            # å‰µå»ºåœ–è¡¨
            plt.figure(figsize=(12, 6))
            plt.plot(dates, prices, label='Close Price', color='blue', linewidth=1.5)
            
            # è¨­ç½®åœ–è¡¨å±¬æ€§
            plt.title("Stock Price History (Close Price Only)", fontsize=14, fontweight='bold')
            plt.xlabel("Date")
            plt.ylabel("Price ($)")
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            # å¦‚æœæ—¥æœŸå¤ªå¤šï¼Œè‡ªå‹•èª¿æ•´xè»¸æ¨™ç±¤
            if len(dates) > 50:
                plt.xticks(rotation=45)
                # åªé¡¯ç¤ºéƒ¨åˆ†æ—¥æœŸæ¨™ç±¤
                step = max(1, len(dates) // 10)
                selected_dates = dates[::step]
                plt.xticks(selected_dates, rotation=45)
            
            plt.tight_layout()
            
            # åœ¨åœ–ä¸Šæ·»åŠ çµ±è¨ˆä¿¡æ¯ - ç¾åœ¨ä½¿ç”¨ç´”æ•¸å€¼
            stats_text = f'Start: ${start_price:.2f}\nEnd: ${end_price:.2f}\nReturn: {total_return:.2f}%'
            plt.text(0.02, 0.98, stats_text, transform=plt.gca().transAxes, 
                    verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
            
            plt.show()
            print("Stock price history plotted successfully.")
            
            # æ‰“å°æ•¸æ“šä¿¡æ¯ç”¨æ–¼èª¿è©¦
            print(f"Data shape: {self.unprocessed_data.shape}")
            print(f"Columns: {list(self.unprocessed_data.columns)}")
            print(f"Index name: {self.unprocessed_data.index.name}")
            print(f"Price range: ${start_price:.2f} - ${end_price:.2f}")
            print(f"Total return: {total_return:.2f}%")
            
        except Exception as e:
            print(f"Error plotting stock history: {e}")
            print(f"Data columns: {list(self.unprocessed_data.columns)}")
            print(f"Data shape: {self.unprocessed_data.shape}")
            print(f"Index: {self.unprocessed_data.index}")
            import traceback
            traceback.print_exc()
    

    def testing_process(self):
        """
        åŸ·è¡Œ MADDQN æ¸¬è©¦éç¨‹ - ä¿®æ­£ç´¯ç©å›å ±è¨ˆç®—
        """
        print("é–‹å§‹ MADDQN æ¸¬è©¦éç¨‹...")
        
        
        
        # é‡ç½®ç’°å¢ƒ
        self.initialize()
        self.env.reset()
        for subEnv in self.subEnvList:
            subEnv.reset()
        # ç²å–æ¸¬è©¦æ•¸æ“šç¸½é•·åº¦
        total_sequences = len(self.test_data)
        print(f"Total testing sequences: {total_sequences}")

        # choose the most recently model all agents
        model_paths_pattern = glob.glob(os.path.join(self.configs['training']['model_save_dir'], "*agent_*.pth"))
        risk_agent_model_path =  [path for path in model_paths_pattern if path.startswith("./checkpoints/risk_agent")]
        return_agent_model_path = [path for path in model_paths_pattern if path.startswith("./checkpoints/return_agent")]
        final_agent_model_path = [path for path in model_paths_pattern if path.startswith("./checkpoints/final_agent")]
        sorted(risk_agent_model_path, key=os.path.getmtime)
        sorted(return_agent_model_path, key=os.path.getmtime)
        sorted(final_agent_model_path, key=os.path.getmtime)
        model_path = [
            risk_agent_model_path[0] if risk_agent_model_path else None,
            return_agent_model_path[0] if return_agent_model_path else None,
            final_agent_model_path[0] if final_agent_model_path else None
        ]
        try:
            self.load_models(model_paths=model_path)
            print("æ¨¡å‹åŠ è¼‰æˆåŠŸï¼Œé–‹å§‹æ¸¬è©¦éç¨‹...")
        except Exception as e:
            print(f"æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            return

        # åˆå§‹åŒ–è®Šé‡
        num_agents = self.configs['env']['risk_agent'] + self.configs['env']['return_agent']
        QValues_List = np.zeros((num_agents, total_sequences, 3), dtype=np.float32)
        
        # ä¿®æ­£ï¼šåˆå§‹åŒ–æŠ•è³‡çµ„åˆè·Ÿè¹¤è®Šé‡
        initial_balance = float(self.configs['env']['initial_balance'])
        
        # ç´¯ç©å›å ±è¿½è¹¤ - ä»¥ç™¾åˆ†æ¯”å½¢å¼ç´¯ç©
        cumulative_returns = {
            'risk_agent': 0.0,
            'return_agent': 0.0,
            'final_agent': 0.0
        }
        
        # æŠ•è³‡çµ„åˆåƒ¹å€¼è¿½è¹¤ - ä»¥å¯¦éš›é‡‘é¡è¿½è¹¤
        portfolio_values = {
            'risk_agent': initial_balance,
            'return_agent': initial_balance,
            'final_agent': initial_balance
        }
        
        # å–®æ­¥çå‹µç´¯ç©
        episode_rewards = {
            'risk_agent': 0.0,
            'return_agent': 0.0,
            'final_agent': 0.0
        }
        
        # ä¸»æ¸¬è©¦å¾ªç’°
        timestep_pbar = tqdm(
            range(total_sequences), 
            desc="Testing", 
            unit="step",
            ncols=200,
            leave=True,
            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}] {postfix}'
        )
        
        # è¨˜éŒ„ç¬¬ä¸€å€‹åƒ¹æ ¼ä½œç‚ºåŸºæº–
        first_price = None
        
        for timestep in timestep_pbar:
            if timestep >= total_sequences:
                break
            
            # ç²å–ç•¶å‰æ•¸æ“š
            current_sequence = self.test_data[timestep]
            # test_start_idx = len(self.train_data)
            # current_data_index = test_start_idx + timestep
            actual_test_start_idx = self.configs['env']['window_size'] - 1 + len(self.train_data)
            current_data_index = actual_test_start_idx + timestep
            
            if current_data_index >= len(self.unprocessed_data):
                current_data_index = len(self.unprocessed_data) - 1
            
            # ç²å–ç•¶å‰åƒ¹æ ¼
            current_price = self.unprocessed_data.iloc[current_data_index]['Close']
            if hasattr(current_price, 'item'):
                current_price = current_price.item()
            else:
                current_price = float(current_price)
            
            # è¨˜éŒ„ç¬¬ä¸€å€‹åƒ¹æ ¼
            if first_price is None:
                first_price = current_price
            
            current_date = self.unprocessed_data.index[current_data_index]
            preprocessed_state = torch.tensor(current_sequence, dtype=torch.float32).to(self.device)
            
            # æ›´æ–°ç’°å¢ƒç‹€æ…‹
            self.env.state = current_sequence
            
            # ç•¶å‰æ™‚é–“æ­¥çš„çå‹µ
            timestep_rewards = {
                'risk_agent': 0.0,
                'return_agent': 0.0,
                'final_agent': 0.0
            }
            
            # å­ä»£ç†è™•ç†å¾ªç’°
            for agentIndex in range(num_agents):
                # ç¢ºå®šä»£ç†é¡å‹å’Œæ¨¡å‹
                if agentIndex < self.configs['env']['risk_agent']:
                    model = self.riskAgentModel
                    agent = 'risk'
                    subAgentEnv = self.subEnvList[0]
                    agent_key = 'risk_agent'
                else:
                    model = self.returnAgentModel
                    agent = 'return'
                    subAgentEnv = self.subEnvList[1]
                    agent_key = 'return_agent'
                
                # ä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹é€²è¡Œé æ¸¬
                with torch.no_grad():
                    action, QValues = model.act(preprocessed_state, training=False)
                    QValues_List[agentIndex, timestep, :] = QValues
                    
                    # ç¢ºä¿å‹•ä½œåœ¨æ­£ç¢ºç¯„åœå…§
                    if action not in [-1, 0, 1]:
                        if action in [0, 1, 2]:
                            action_mapping = {0: -1, 1: 0, 2: 1}
                            action = action_mapping[action]
                        else:
                            action = 0

                # åŸ·è¡Œå‹•ä½œä¸¦ç²å–çå‹µ
                next_state, reward, done, info = subAgentEnv.step(
                    action, agentType=agent, tradeAmount=self.configs['env']['trade_amount']
                )
                
                # ç´¯ç©çå‹µ
                timestep_rewards[agent_key] += reward
                episode_rewards[agent_key] += reward
            
            # Final Agentè™•ç†
            current_subagent_qvalues = QValues_List[:, timestep, :]
            
            with torch.no_grad():
                finalAgent_action, final_q_values = self.finalAgentModel.act(
                    current_sequence, current_subagent_qvalues, training=False
                )
                
                # è™•ç† Final Agent å‹•ä½œ
                if isinstance(finalAgent_action, torch.Tensor):
                    if finalAgent_action.dim() == 0:
                        final_action = finalAgent_action.item()
                    else:
                        final_action = finalAgent_action.argmax().item()
                else:
                    final_action = finalAgent_action
                
                # ç¢ºä¿å‹•ä½œæœ‰æ•ˆ
                if final_action in [0, 1, 2]:
                    action_mapping = {0: -1, 1: 0, 2: 1}
                    final_action = action_mapping[final_action]
                elif final_action not in [-1, 0, 1]:
                    final_action = 0
            
            # Final agentåŸ·è¡Œå‹•ä½œ
            final_next_state, final_reward, final_done, info = self.env.step(
                final_action, agentType='final', tradeAmount=self.configs['env']['trade_amount']
            )
            
            timestep_rewards['final_agent'] = final_reward
            episode_rewards['final_agent'] += final_reward
            
            # ä¿®æ­£ï¼šè¨ˆç®—ç´¯ç©å›å ±ç‡
            # æ–¹æ³•1: åŸºæ–¼çå‹µçš„ç´¯ç©å›å ±ç‡
            for agent_key in ['risk_agent', 'return_agent', 'final_agent']:
                # å°‡çå‹µè½‰æ›ç‚ºå›å ±ç‡ï¼ˆå‡è¨­çå‹µå·²ç¶“æ˜¯ç™¾åˆ†æ¯”å½¢å¼ï¼‰
                step_return_pct = timestep_rewards[agent_key]
                
                # ç´¯ç©å›å ±ç‡è¨ˆç®—ï¼š(1 + r1) * (1 + r2) * ... - 1
                cumulative_returns[agent_key] = ((1 + cumulative_returns[agent_key]/100) * 
                                            (1 + step_return_pct/100) - 1) * 100
                
                # æ›´æ–°æŠ•è³‡çµ„åˆåƒ¹å€¼
                portfolio_values[agent_key] = initial_balance * (1 + cumulative_returns[agent_key]/100)
            
            # æ–¹æ³•2: åŸºæ–¼åƒ¹æ ¼è®Šå‹•çš„ç´¯ç©å›å ±ç‡ï¼ˆä½œç‚ºåƒè€ƒï¼‰
            market_return = ((current_price - first_price) / first_price) * 100
            
            # æ›´æ–°é€²åº¦æ¢ä¿¡æ¯
            timestep_pbar.set_postfix({
                'Risk': f'{cumulative_returns["risk_agent"]:.3f}%',
                'Return': f'{cumulative_returns["return_agent"]:.3f}%',
                'Final': f'{cumulative_returns["final_agent"]:.3f}%',
                'Market': f'{market_return:.3f}%',
                'Price': f'{current_price:.0f}'
            })
        
        print("\nMADDQN æ¸¬è©¦éç¨‹å®Œæˆï¼")
        
        # æ‰“å°è©³ç´°çµæœ
        print("\n" + "="*80)
        print("ğŸ“Š æ¸¬è©¦çµæœç¸½çµ")
        print("="*80)
        
        print(f"\nğŸ“ˆ æœŸé–“åƒ¹æ ¼è®Šå‹•:")
        print(f"   èµ·å§‹åƒ¹æ ¼: ${first_price:.2f}")
        print(f"   çµæŸåƒ¹æ ¼: ${current_price:.2f}")
        print(f"   å¸‚å ´å›å ±: {market_return:.4f}%")
        
        print(f"\nğŸ’° å„Agentè¡¨ç¾:")
        for agent_key in ['risk_agent', 'return_agent', 'final_agent']:
            agent_name = agent_key.replace('_', ' ').title()
            print(f"   {agent_name}:")
            print(f"     ç¸½çå‹µ: {episode_rewards[agent_key]:.6f}")
            print(f"     ç´¯ç©å›å ±ç‡: {cumulative_returns[agent_key]:.4f}%")
            print(f"     æœ€çµ‚åƒ¹å€¼: ${portfolio_values[agent_key]:,.2f}")
            print(f"     ç›¸å°å¸‚å ´: {cumulative_returns[agent_key] - market_return:.4f}%")
            print()
        
        # æ‰¾å‡ºæœ€ä½³è¡¨ç¾
        best_agent = max(cumulative_returns.keys(), key=lambda k: cumulative_returns[k])
        worst_agent = min(cumulative_returns.keys(), key=lambda k: cumulative_returns[k])
        
        print(f"ğŸ† æœ€ä½³è¡¨ç¾: {best_agent.replace('_', ' ').title()} ({cumulative_returns[best_agent]:.4f}%)")
        print(f"ğŸ“‰ æœ€å·®è¡¨ç¾: {worst_agent.replace('_', ' ').title()} ({cumulative_returns[worst_agent]:.4f}%)")
        
        # è¨ˆç®—Alpha (ç›¸å°å¸‚å ´çš„è¶…é¡å›å ±)
        print(f"\nğŸ“Š Alphaåˆ†æ (ç›¸å°å¸‚å ´çš„è¶…é¡å›å ±):")
        for agent_key in ['risk_agent', 'return_agent', 'final_agent']:
            alpha = cumulative_returns[agent_key] - market_return
            agent_name = agent_key.replace('_', ' ').title()
            status = "ğŸ”¥ è¶…è¶Šå¸‚å ´" if alpha > 0 else "ğŸ“‰ è·‘è¼¸å¸‚å ´" if alpha < 0 else "âš–ï¸ æŒå¹³å¸‚å ´"
            print(f"   {agent_name}: {alpha:.4f}% {status}")
        
        print("="*80)
        
        # ä¿å­˜çµæœåˆ°å±¬æ€§ä¸­ï¼Œä¾›å¾ŒçºŒç¹ªåœ–ä½¿ç”¨
        self.test_cumulative_returns = cumulative_returns
        self.test_portfolio_values_final = portfolio_values
        self.test_episode_rewards = episode_rewards
        self.market_return = market_return
        
        # ç¹ªè£½æ¸¬è©¦çµæœ
        self.plot_test_results()
        self.save_test_statistics()
        
        return {
            'cumulative_returns': cumulative_returns,
            'portfolio_values': portfolio_values,
            'episode_rewards': episode_rewards,
            'market_return': market_return
        }

    def plot_test_results(self):
        """
        ç¹ªè£½æ¸¬è©¦çµæœåœ–è¡¨
        """
        try:
            # æª¢æŸ¥å¿…éœ€çš„å±¬æ€§æ˜¯å¦å­˜åœ¨
            if not hasattr(self, 'test_cumulative_returns'):
                print("âš ï¸ æ¸¬è©¦çµæœæ•¸æ“šä¸å­˜åœ¨ï¼Œç„¡æ³•ç¹ªåœ–")
                return
            
            # å‰µå»ºåœ–è¡¨
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle('MADDQN Testing Results Analysis', fontsize=16, fontweight='bold')
            
            # 1. ç´¯ç©å›å ±ç‡æ¯”è¼ƒ
            agents = list(self.test_cumulative_returns.keys())
            returns = list(self.test_cumulative_returns.values())
            returns.append(self.market_return)  # æ·»åŠ å¸‚å ´å›å ±
            agents.append('Market')
            
            colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']
            bars = ax1.bar(range(len(agents)), returns, color=colors)
            ax1.set_title('Cumulative Returns Comparison', fontweight='bold')
            ax1.set_ylabel('Return (%)')
            ax1.set_xticks(range(len(agents)))
            ax1.set_xticklabels([agent.replace('_', ' ').title() for agent in agents], rotation=45)
            ax1.grid(True, alpha=0.3)
            
            # æ·»åŠ æ•¸å€¼æ¨™ç±¤
            for bar, value in zip(bars, returns):
                height = bar.get_height()
                ax1.text(bar.get_x() + bar.get_width()/2., height,
                        f'{value:.3f}%', ha='center', va='bottom' if height >= 0 else 'top')
            
            # 2. æŠ•è³‡çµ„åˆåƒ¹å€¼å°æ¯”
            portfolio_values = list(self.test_portfolio_values_final.values())
            ax2.bar(range(len(agents)-1), portfolio_values, color=colors[:-1])
            ax2.set_title('Final Portfolio Values', fontweight='bold')
            ax2.set_ylabel('Value ($)')
            ax2.set_xticks(range(len(agents)-1))
            ax2.set_xticklabels([agent.replace('_', ' ').title() for agent in agents[:-1]], rotation=45)
            ax2.grid(True, alpha=0.3)
            
            # æ·»åŠ åˆå§‹å€¼ç·š
            initial_balance = float(self.configs['env']['initial_balance'])
            ax2.axhline(y=initial_balance, color='red', linestyle='--', alpha=0.7, label=f'Initial: ${initial_balance:,.0f}')
            ax2.legend()
            
            # 3. Alphaåˆ†æï¼ˆç›¸å°å¸‚å ´è¶…é¡å›å ±ï¼‰
            alphas = [self.test_cumulative_returns[agent] - self.market_return 
                    for agent in ['risk_agent', 'return_agent', 'final_agent']]
            agent_names = [agent.replace('_', ' ').title() for agent in ['risk_agent', 'return_agent', 'final_agent']]
            
            colors_alpha = ['green' if alpha > 0 else 'red' for alpha in alphas]
            bars_alpha = ax3.bar(range(len(agent_names)), alphas, color=colors_alpha, alpha=0.7)
            ax3.set_title('Alpha (Excess Return vs Market)', fontweight='bold')
            ax3.set_ylabel('Alpha (%)')
            ax3.set_xticks(range(len(agent_names)))
            ax3.set_xticklabels(agent_names, rotation=45)
            ax3.axhline(y=0, color='black', linestyle='-', alpha=0.5)
            ax3.grid(True, alpha=0.3)
            
            # æ·»åŠ æ•¸å€¼æ¨™ç±¤
            for bar, value in zip(bars_alpha, alphas):
                height = bar.get_height()
                ax3.text(bar.get_x() + bar.get_width()/2., height,
                        f'{value:.3f}%', ha='center', va='bottom' if height >= 0 else 'top')
            
            # 4. ç¸½çå‹µå°æ¯”
            total_rewards = list(self.test_episode_rewards.values())
            ax4.bar(range(len(agent_names)), total_rewards, color=colors[:-1])
            ax4.set_title('Total Episode Rewards', fontweight='bold')
            ax4.set_ylabel('Total Reward')
            ax4.set_xticks(range(len(agent_names)))
            ax4.set_xticklabels(agent_names, rotation=45)
            ax4.grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            # ä¿å­˜åœ–è¡¨
            plt.savefig('maddqn_test_results.png', dpi=300, bbox_inches='tight')
            plt.show()
            
            print("ğŸ“Š æ¸¬è©¦çµæœåœ–è¡¨å·²ä¿å­˜ç‚º 'maddqn_test_results.png'")
            
        except Exception as e:
            print(f"ç¹ªåœ–éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()

    def save_test_statistics(self):
        """
        ä¿å­˜æ¸¬è©¦çµ±è¨ˆçµæœåˆ°æ–‡ä»¶
        """
        try:
            # ç²å–æ¸¬è©¦æœŸé–“çš„å¯¦éš›æ—¥æœŸå’Œåƒ¹æ ¼
            test_start_idx = len(self.train_data)
            test_end_idx = test_start_idx + len(self.test_data) - 1
            
            # ç¢ºä¿ç´¢å¼•ä¸è¶…å‡ºç¯„åœ
            test_end_idx = min(test_end_idx, len(self.unprocessed_data) - 1)
            
            # ç²å–æ¸¬è©¦æœŸé–“çš„æ—¥æœŸ
            test_start_date = self.unprocessed_data.index[test_start_idx]
            test_end_date = self.unprocessed_data.index[test_end_idx]
            
            # ç²å–æ¸¬è©¦æœŸé–“çš„åƒ¹æ ¼
            # è™•ç†MultiIndexåˆ—çµæ§‹
            if ('Close', 'DJI') in self.unprocessed_data.columns:
                close_col = ('Close', 'DJI')
            else:
                # æ‰¾åˆ°Closeç›¸é—œçš„åˆ—
                close_col = None
                for col in self.unprocessed_data.columns:
                    if isinstance(col, tuple) and col[0] == 'Close':
                        close_col = col
                        break
                    elif col == 'Close':
                        close_col = col
                        break
            
            if close_col:
                test_start_price = float(self.unprocessed_data.iloc[test_start_idx][close_col])
                test_end_price = float(self.unprocessed_data.iloc[test_end_idx][close_col])
            else:
                test_start_price = 0.0
                test_end_price = 0.0
            
            # æº–å‚™çµ±è¨ˆæ•¸æ“š
            stats = {
                'test_date': datetime.now().isoformat(),
                'test_period': {
                    'start_date': str(test_start_date),
                    'end_date': str(test_end_date),
                    'total_days': len(self.test_data)
                },
                'market_data': {
                    'start_price': test_start_price,
                    'end_price': test_end_price,
                    'market_return': float(self.market_return)
                },
                'agent_performance': {
                    agent: {
                        'cumulative_return_pct': float(self.test_cumulative_returns[agent]),
                        'total_rewards': float(self.test_episode_rewards[agent]),
                        'final_portfolio_value': float(self.test_portfolio_values_final[agent]),
                        'alpha_vs_market': float(self.test_cumulative_returns[agent] - self.market_return)
                    }
                    for agent in ['risk_agent', 'return_agent', 'final_agent']
                },
                'performance_metrics': {
                    'best_agent': max(self.test_cumulative_returns.keys(), 
                                    key=lambda k: self.test_cumulative_returns[k]),
                    'worst_agent': min(self.test_cumulative_returns.keys(), 
                                    key=lambda k: self.test_cumulative_returns[k]),
                    'agents_beating_market': [agent for agent in ['risk_agent', 'return_agent', 'final_agent'] 
                                            if self.test_cumulative_returns[agent] > self.market_return]
                },
                'config_used': {
                    'data': self.configs['data'],
                    'env': self.configs['env'],
                    'training': {k: v for k, v in self.configs['training'].items() 
                            if k not in ['risk_agent_model_path', 'return_agent_model_path', 'final_agent_model_path']}
                }
            }
            
            # ä¿å­˜åˆ°JSONæ–‡ä»¶
            filename = f"maddqn_test_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(stats, f, indent=2, ensure_ascii=False, default=str)
            
            print(f"ğŸ“„ æ¸¬è©¦çµ±è¨ˆçµæœå·²ä¿å­˜ç‚º '{filename}'")
            
            # æ‰“å°é—œéµçµ±è¨ˆä¿¡æ¯
            print(f"\nğŸ“‹ é—œéµçµ±è¨ˆä¿¡æ¯:")
            print(f"   æ¸¬è©¦æœŸé–“: {test_start_date.date()} è‡³ {test_end_date.date()}")
            print(f"   æ¸¬è©¦å¤©æ•¸: {len(self.test_data)} å¤©")
            print(f"   å¸‚å ´å›å ±: {self.market_return:.4f}%")
            print(f"   æœ€ä½³Agent: {stats['performance_metrics']['best_agent'].replace('_', ' ').title()}")
            print(f"   è¶…è¶Šå¸‚å ´çš„Agentæ•¸é‡: {len(stats['performance_metrics']['agents_beating_market'])}")
            
        except Exception as e:
            print(f"ä¿å­˜çµ±è¨ˆçµæœæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
    
    def load_models(self, model_paths):
        """
        è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹
        
        Args:
            model_paths: åŒ…å«ä¸‰å€‹æ¨¡å‹è·¯å¾‘çš„åˆ—è¡¨ [risk_agent_path, return_agent_path, final_agent_path]
        """
        try:
            risk_agent_path, return_agent_path, final_agent_path = model_paths
            
            # è¼‰å…¥ Risk Agent æ¨¡å‹
            if risk_agent_path and os.path.exists(risk_agent_path):
                risk_state_dict = torch.load(risk_agent_path, map_location=self.device, weights_only=True)
                self.riskAgentModel.policy_network.load_state_dict(risk_state_dict)
                self.riskAgentModel.target_network.load_state_dict(risk_state_dict)
                print(f"âœ… Risk Agent æ¨¡å‹å·²è¼‰å…¥: {risk_agent_path}")
            else:
                print(f"âš ï¸ Risk Agent æ¨¡å‹è·¯å¾‘ç„¡æ•ˆæˆ–æª”æ¡ˆä¸å­˜åœ¨: {risk_agent_path}")
                
            # è¼‰å…¥ Return Agent æ¨¡å‹
            if return_agent_path and os.path.exists(return_agent_path):
                return_state_dict = torch.load(return_agent_path, map_location=self.device, weights_only=True)
                self.returnAgentModel.policy_network.load_state_dict(return_state_dict)
                self.returnAgentModel.target_network.load_state_dict(return_state_dict)
                print(f"âœ… Return Agent æ¨¡å‹å·²è¼‰å…¥: {return_agent_path}")
            else:
                print(f"âš ï¸ Return Agent æ¨¡å‹è·¯å¾‘ç„¡æ•ˆæˆ–æª”æ¡ˆä¸å­˜åœ¨: {return_agent_path}")
                
            # è¼‰å…¥ Final Agent æ¨¡å‹
            if final_agent_path and os.path.exists(final_agent_path):
                final_state_dict = torch.load(final_agent_path, map_location=self.device, weights_only=True)
                self.finalAgentModel.policy_network.load_state_dict(final_state_dict)
                self.finalAgentModel.target_network.load_state_dict(final_state_dict)
                print(f"âœ… Final Agent æ¨¡å‹å·²è¼‰å…¥: {final_agent_path}")
            else:
                print(f"âš ï¸ Final Agent æ¨¡å‹è·¯å¾‘ç„¡æ•ˆæˆ–æª”æ¡ˆä¸å­˜åœ¨: {final_agent_path}")
            
            # è¨­ç½®ç‚ºè©•ä¼°æ¨¡å¼
            self.riskAgentModel.policy_network.eval()
            self.riskAgentModel.target_network.eval()
            self.returnAgentModel.policy_network.eval()
            self.returnAgentModel.target_network.eval()
            self.finalAgentModel.policy_network.eval()
            self.finalAgentModel.target_network.eval()
            
            print("ğŸ“‹ æ‰€æœ‰æ¨¡å‹å·²è¨­ç½®ç‚ºè©•ä¼°æ¨¡å¼")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹è¼‰å…¥éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
            raise e

    def save_models(self, episode=None):
        """
        ä¿å­˜è¨“ç·´å¥½çš„æ¨¡å‹
        
        Args:
            episode: ç•¶å‰episodeæ•¸ (å¯é¸ï¼Œç”¨æ–¼æª”å)
        """
        try:
            
            # å‰µå»ºæ¨¡å‹ä¿å­˜ç›®éŒ„
            save_dir = self.configs['training']['model_save_dir']
            
            # ç”Ÿæˆæª”å
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            episode_suffix = f"_ep{episode}" if episode else ""
            
            # ä¿å­˜ Risk Agent
            risk_path = os.path.join(save_dir, f'risk_agent{episode_suffix}_{timestamp}.pth')
            torch.save(self.riskAgentModel.policy_network.state_dict(), risk_path)
            print(f"âœ… Risk Agent æ¨¡å‹å·²ä¿å­˜: {risk_path}")
            
            # ä¿å­˜ Return Agent  
            return_path = os.path.join(save_dir, f'return_agent{episode_suffix}_{timestamp}.pth')
            torch.save(self.returnAgentModel.policy_network.state_dict(), return_path)
            print(f"âœ… Return Agent æ¨¡å‹å·²ä¿å­˜: {return_path}")
            
            # ä¿å­˜ Final Agent
            final_path = os.path.join(save_dir, f'final_agent{episode_suffix}_{timestamp}.pth')
            torch.save(self.finalAgentModel.policy_network.state_dict(), final_path)
            print(f"âœ… Final Agent æ¨¡å‹å·²ä¿å­˜: {final_path}")
            
            
            return {
                'risk_agent_path': risk_path,
                'return_agent_path': return_path,
                'final_agent_path': final_path
            }
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹ä¿å­˜éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
            raise e