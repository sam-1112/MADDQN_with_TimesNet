import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class InputBlock:
    def __init__(self, tradeData, Qvalue_sub_list):
        self.tradeData = tradeData
        self.Qvalue_sub_list = Qvalue_sub_list
    
    def getInput(self, block='MultiScaleBlock'):
        """ 
        Prepare the input tensor by processing trade data and Q-value sub-tensors.
        
        :return input_data: Processed tensor based on block type
        """
        if block == 'MultiScaleBlock':
            # å°æ–¼ MultiScaleBlockï¼Œåªè¿”å›å¸‚å ´æ•¸æ“š
            # å› ç‚ºå¸‚å ´æ•¸æ“šå’Œå­ä»£ç†Qå€¼çš„ç¶­åº¦ä¸å…¼å®¹ï¼Œç„¡æ³•ç›´æ¥é€£æ¥
            return self.tradeData
        elif block == 'ActionBlock':
            # å°æ–¼ ActionBlockï¼Œè¿”å›å±•å¹³çš„å­ä»£ç†Qå€¼
            if isinstance(self.Qvalue_sub_list, list) and len(self.Qvalue_sub_list) > 0:
                # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œé€£æ¥æ‰€æœ‰Qå€¼
                if len(self.Qvalue_sub_list) == 1:
                    qvalues = self.Qvalue_sub_list[0]
                else:
                    qvalues = torch.cat(self.Qvalue_sub_list, dim=-1)
            else:
                # å¦‚æœä¸æ˜¯åˆ—è¡¨ï¼Œç›´æ¥ä½¿ç”¨
                qvalues = self.Qvalue_sub_list[0] if isinstance(self.Qvalue_sub_list, list) else self.Qvalue_sub_list
            
            # ç¢ºä¿æœ‰æ­£ç¢ºçš„batchç¶­åº¦
            if qvalues.dim() == 2:  # (n_agents, 3)
                qvalues = qvalues.unsqueeze(0)  # (1, n_agents, 3)
            
            # å±•å¹³ç‚º (batch_size, n_agents * 3)
            batch_size = qvalues.size(0)
            flattened_qvalues = qvalues.view(batch_size, -1)
            return flattened_qvalues
        else:
            raise ValueError("Unsupported block type. Use 'MultiScaleBlock' or 'ActionBlock'.")

class MultiScaleBlock(nn.Module):
    def __init__(self, input_channels=5, sequence_length=10):
        super(MultiScaleBlock, self).__init__()
        
        # ç¬¬ä¸€å€‹æ¨¡å¡Šï¼šå–®å°ºåº¦ç‰¹å¾µæå–å™¨
        # ä½¿ç”¨ä¸€ç¶­å·ç©æŠ€è¡“æå–æ™‚åŸŸç‰¹å¾µï¼Œè¼¸å‡º5ç¶­ç‰¹å¾µ
        self.single_scale = nn.Conv1d(
            in_channels=input_channels,  # 5å€‹åƒæ•¸ (OHLCV)
            out_channels=5,  # å †ç–Šæˆ5ç¶­ç‰¹å¾µ
            kernel_size=3,
            padding=1
        )
        
        # ç¬¬äºŒå€‹æ¨¡å¡Šï¼šä¸­ç­‰å°ºåº¦ç‰¹å¾µæå–å™¨
        # å°‡æ•¸æ“šè¦–ç‚ºå–®é€šé“äºŒç¶­åœ–åƒï¼Œä½¿ç”¨3Ã—3å·ç©æ ¸çš„äºŒç¶­å·ç©
        self.medium_scale = nn.Conv2d(
            in_channels=1,  # å–®é€šé“
            out_channels=2,  # 2å€‹è¼¸å‡ºé€šé“
            kernel_size=(3, 3),
            padding=(1, 1)
        )
        
        # ç¬¬ä¸‰å€‹æ¨¡å¡Šï¼šå…¨å±€å°ºåº¦ç‰¹å¾µæå–å™¨
        # è€ƒæ…®å–®é€šé“åœ–åƒæ•¸æ“šï¼Œä½¿ç”¨5Ã—5å·ç©æ ¸å¤§å°é€²è¡Œå…¨å±€ç‰¹å¾µæå–
        self.global_scale = nn.Conv2d(
            in_channels=1,  # å–®é€šé“
            out_channels=1,  # 1å€‹è¼¸å‡ºé€šé“
            kernel_size=(5, 5),
            padding=(2, 2)
        )
        
        self.sequence_length = sequence_length
        self.input_channels = input_channels
    
    def forward(self, x):
        """
        è¼¸å…¥: (batch_size, 1, sequence_length, input_channels)
        è¼¸å‡º: (batch_size, 8, sequence_length, input_channels) - 8é€šé“äºŒç¶­ç‰¹å¾µå±¤
        """
        batch_size = x.size(0)
        
        # ç§»é™¤é€šé“ç¶­åº¦ä»¥ä¾¿1Då·ç©: (batch_size, sequence_length, input_channels)
        x_for_1d = x.squeeze(1)  # (batch_size, sequence_length, input_channels)
        
        # è½‰ç½®ç‚º1Då·ç©æœŸæœ›çš„æ ¼å¼: (batch_size, input_channels, sequence_length)
        x_for_1d = x_for_1d.transpose(1, 2)  # (batch_size, input_channels, sequence_length)
        
        # ç¬¬ä¸€å€‹æ¨¡å¡Šï¼šå–®å°ºåº¦ç‰¹å¾µæå– (1Då·ç©)
        single_features = F.relu(self.single_scale(x_for_1d))  # (batch_size, 5, sequence_length)
        
        # å°‡1Då·ç©çµæœé‡å¡‘ç‚º2Dæ ¼å¼ä»¥ä¾¿é€£æ¥
        # (batch_size, 5, sequence_length) -> (batch_size, 5, sequence_length, input_channels)
        single_features_2d = single_features.unsqueeze(-1).expand(
            -1, -1, -1, self.input_channels
        )  # (batch_size, 5, sequence_length, input_channels)
        
        # ç¬¬äºŒå€‹æ¨¡å¡Šï¼šä¸­ç­‰å°ºåº¦ç‰¹å¾µæå– (2Då·ç©ï¼Œ3Ã—3æ ¸)
        medium_features = F.relu(self.medium_scale(x))  # (batch_size, 2, sequence_length, input_channels)
        
        # ç¬¬ä¸‰å€‹æ¨¡å¡Šï¼šå…¨å±€å°ºåº¦ç‰¹å¾µæå– (2Då·ç©ï¼Œ5Ã—5æ ¸)
        global_features = F.relu(self.global_scale(x))  # (batch_size, 1, sequence_length, input_channels)
        
        # é€£æ¥ä¸‰å€‹æ¨¡å¡Šçš„è¼¸å‡ºå½¢æˆ8é€šé“äºŒç¶­ç‰¹å¾µå±¤
        # 5 (single) + 2 (medium) + 1 (global) = 8 é€šé“
        multi_scale_output = torch.cat([
            single_features_2d,  # (batch_size, 5, sequence_length, input_channels)
            medium_features,     # (batch_size, 2, sequence_length, input_channels)
            global_features      # (batch_size, 1, sequence_length, input_channels)
        ], dim=1)  # (batch_size, 8, sequence_length, input_channels)
        
        return multi_scale_output


class ECABlock2D(nn.Module):
    """2Dç‰ˆæœ¬çš„ECA Blockï¼Œé©ç”¨æ–¼åœ–åƒæ•¸æ“š"""
    def __init__(self, channels, gamma=2, b=1):
        super(ECABlock2D, self).__init__()
        # è‡ªé©æ‡‰æ ¸å¤§å°
        kernel_size = int(abs(math.log2(channels) / gamma + b / gamma))
        kernel_size = kernel_size if kernel_size % 2 else kernel_size + 1
        
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.conv = nn.Conv1d(1, 1, kernel_size=kernel_size, 
                             padding=kernel_size // 2, bias=False)
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        # x: (batch_size, channels, height, width)
        batch_size, channels, height, width = x.size()
        
        # å…¨å±€å¹³å‡æ± åŒ–: (batch_size, channels, 1, 1)
        y = self.avg_pool(x)
        
        # é‡å¡‘ç‚º1Då·ç©æ ¼å¼: (batch_size, 1, channels)
        y = y.squeeze(-1).transpose(-1, -2)
        
        # 1Då·ç©
        y = self.conv(y)
        
        # æ¢å¾©å½¢ç‹€ä¸¦æ‡‰ç”¨sigmoid: (batch_size, channels, 1, 1)
        y = y.transpose(-1, -2).unsqueeze(-1)
        y = self.sigmoid(y)
        
        # å»£æ’­ç›¸ä¹˜
        return x * y.expand_as(x)

class BackBoneBlock(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):
        super(BackBoneBlock, self).__init__()
        # ç¬¬ä¸€å±¤å·ç© + MaxPooling
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)
        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)
        
        # ç¬¬äºŒå±¤å·ç©
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, stride, padding)
        
        # ECA Block
        self.eca = ECABlock2D(channels=out_channels)
        
        # ç¬¬ä¸‰å±¤å·ç©
        self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size, stride, padding)
        
        # ç·šæ€§å±¤ï¼ˆç”¨æ–¼æœ€å¾Œçš„ç‰¹å¾µè™•ç†ï¼‰
        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.linear1 = nn.Linear(out_channels, out_channels)
        self.linear2 = nn.Linear(out_channels, out_channels)
    
    def forward(self, x):
        # ç¬¬ä¸€å€‹ Conv3Ã—3 + MaxPooling
        x = F.relu(self.conv1(x))
        x = self.maxpool(x)
        
        # ç¬¬äºŒå€‹ Conv3Ã—3
        x = F.relu(self.conv2(x))
        
        # ECA Block
        x = self.eca(x)
        
        # ç¬¬ä¸‰å€‹ Conv3Ã—3
        x = F.relu(self.conv3(x))
        
        # å…¨å±€å¹³å‡æ± åŒ–ä¸¦é€šéç·šæ€§å±¤
        pooled = self.adaptive_pool(x)  # (batch_size, out_channels, 1, 1)
        pooled = pooled.view(pooled.size(0), -1)  # (batch_size, out_channels)
        
        features = F.relu(self.linear1(pooled))
        features = self.linear2(features)
        
        return features

class ActionBlock(nn.Module):
    def __init__(self, backbone_features_size, subagent_qvalues_size, hidden_size=64):
        super(ActionBlock, self).__init__()
        self.backbone_features_size = backbone_features_size
        self.subagent_qvalues_size = subagent_qvalues_size
        self.hidden_size = hidden_size
        
        # Action Block æœ‰å…©å€‹ç·šæ€§å±¤
        total_input_size = backbone_features_size + subagent_qvalues_size
        self.linear1 = nn.Linear(total_input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, 3)  # 3å€‹å‹•ä½œ
        self.dropout = nn.Dropout(0.1)
        
        # æ·»åŠ è¼¸å…¥æ­£è¦åŒ–
        self.input_norm = nn.LayerNorm(total_input_size)
        
        # print(f"ActionBlock initialized: backbone={backbone_features_size}, subagent={subagent_qvalues_size}, total={total_input_size}")
    
    def forward(self, backbone_features, subagent_qvalues):
        # é€£æ¥backboneç‰¹å¾µå’Œå­ä»£ç†Qå€¼
        combined = torch.cat([backbone_features, subagent_qvalues], dim=1)
        
        # è¼¸å…¥æ­£è¦åŒ–
        combined = self.input_norm(combined)
        
        # é€šéç·šæ€§å±¤
        x = F.relu(self.linear1(combined))
        x = self.dropout(x)
        x = self.linear2(x)
        
        # é™åˆ¶è¼¸å‡ºç¯„åœé¿å…çˆ†ç‚¸
        x = torch.clamp(x, min=-10.0, max=10.0)
        
        # print(f"ActionBlock forward: output range [{x.min().item():.4f}, {x.max().item():.4f}]")
        return x

class MSCNN(nn.Module):
    def __init__(self, configs):
        super(MSCNN, self).__init__()
        
        # å¾é…ç½®ä¸­ç²å–åƒæ•¸
        multi_scale_config = configs['mscnn']['multi_scale']
        backbone_config = configs['mscnn']['backbone']
        action_config = configs['mscnn']['action']
        
        # æ™‚åºæ•¸æ“šçš„ç¶­åº¦ä¿¡æ¯
        self.sequence_length = configs.get('seq_len', 10)
        self.feature_dim = configs['env']['features']  # 5å€‹ç‰¹å¾µ (OHLCV)
        
        # Multi-Scale Block - æ ¹æ“šè«–æ–‡ç²¾ç¢ºå¯¦ç¾
        self.multi_scale_block = MultiScaleBlock(
            input_channels=self.feature_dim,  # 5å€‹åƒæ•¸
            sequence_length=self.sequence_length
        )
        
        # Multi-Scale Blockè¼¸å‡º8å€‹é€šé“
        multi_scale_output_channels = 8
        
        # Backbone Block (2Då·ç©)
        self.backbone_block = BackBoneBlock(
            multi_scale_output_channels,  # 8é€šé“è¼¸å…¥
            backbone_config['out_channels']
        )
        
        # ä¸åœ¨åˆå§‹åŒ–æ™‚å‰µå»ºActionBlockï¼Œè€Œæ˜¯åœ¨ç¬¬ä¸€æ¬¡forwardæ™‚å‹•æ…‹å‰µå»º
        self.backbone_output_channels = backbone_config['out_channels']
        self.action_hidden_size = action_config['hidden_size']
        self.action_block = None
        
        # è¨ˆç®—å­ä»£ç†Qå€¼çš„å›ºå®šå°ºå¯¸
        # å‡è¨­æœ‰ risk_agent + return_agent å€‹å­ä»£ç†ï¼Œæ¯å€‹è¼¸å‡º3å€‹Qå€¼
        n_risk_agents = configs.get('env', {}).get('risk_agent', 2)
        n_return_agents = configs.get('env', {}).get('return_agent', 2)
        total_agents = n_risk_agents + n_return_agents
        subagent_qvalues_size = total_agents * 3  # æ¯å€‹ä»£ç†3å€‹Qå€¼
        
        # print(f"MSCNN Init - Backbone output: {self.backbone_output_channels}")
        # print(f"MSCNN Init - Subagent Q-values size: {subagent_qvalues_size}")
        # print(f"MSCNN Init - Total agents: {total_agents} (risk: {n_risk_agents}, return: {n_return_agents})")
        
        # ğŸ”§ ä¿®å¾©ï¼šå‰µå»ºå›ºå®šçš„ ActionBlock ä½œç‚ºæ­£å¼çš„å­æ¨¡å¡Š
        self.action_block = ActionBlock(
            backbone_features_size=self.backbone_output_channels,
            subagent_qvalues_size=subagent_qvalues_size,
            hidden_size=self.action_hidden_size
        )
        
        # æ·»åŠ è¼¸å‡ºæ¨™æº–åŒ–å’Œé™åˆ¶
        self.output_norm = nn.LayerNorm(3)
        self.output_scale = nn.Parameter(torch.tensor(1.0))  # å¯å­¸ç¿’çš„è¼¸å‡ºç¸®æ”¾å› å­
        
        # åˆå§‹åŒ–æ‰€æœ‰æ¬Šé‡
        self.apply(self._init_weights)
        
        # print("MSCNN initialized with fixed ActionBlock and output normalization")
    
    def _init_weights(self, m):
        """ä¿å®ˆçš„æ¬Šé‡åˆå§‹åŒ–"""
        if isinstance(m, nn.Linear):
            # ä½¿ç”¨æ›´ä¿å®ˆçš„åˆå§‹åŒ–
            torch.nn.init.xavier_uniform_(m.weight, gain=0.01)
            if m.bias is not None:
                torch.nn.init.constant_(m.bias, 0.0)
        elif isinstance(m, (nn.Conv1d, nn.Conv2d)):
            torch.nn.init.xavier_uniform_(m.weight, gain=0.01)
            if m.bias is not None:
                torch.nn.init.constant_(m.bias, 0.0)
    
    def forward(self, market_data, sub_agent_qvalues):
        """
        å‰å‘å‚³æ’­
        
        Args:
            market_data: [B, T, N] æˆ– [T, N] å¸‚å ´æ•¸æ“š
            sub_agent_qvalues: [B, n_agents, 3] æˆ– [n_agents, 3] å­ä»£ç†Qå€¼
        
        Returns:
            torch.Tensor: [B, 3] Qå€¼è¼¸å‡º
        """
        try:
            # ğŸ”§ ä¿®å¾©ï¼šæ¨™æº–åŒ–è¼¸å…¥ç¶­åº¦
            market_data, sub_agent_qvalues = self._standardize_inputs(market_data, sub_agent_qvalues)
            batch_size = market_data.size(0)
            
            # Multi-Scaleç‰¹å¾µæå–
            market_data_2d = market_data.unsqueeze(1)  # [B, 1, T, N]
            multi_scale_features = self.multi_scale_block(market_data_2d)  # [B, 8, T, N]
            
            # Backboneè™•ç†
            backbone_features = self.backbone_block(multi_scale_features)  # [B, out_channels]
            
            # æª¢æŸ¥backboneç‰¹å¾µ
            if torch.isnan(backbone_features).any() or torch.isinf(backbone_features).any():
                print("Warning: backbone_features contains NaN or Inf")
                return torch.zeros(batch_size, 3, device=market_data.device)
            
            # ğŸ”§ ä¿®å¾©ï¼šå›ºå®šçš„å­ä»£ç†Qå€¼è™•ç†
            subagent_flat = self._process_subagent_qvalues(sub_agent_qvalues, batch_size)
            
            # æª¢æŸ¥ç¶­åº¦åŒ¹é…
            expected_backbone_size = self.action_block.backbone_features_size
            expected_subagent_size = self.action_block.subagent_qvalues_size
            actual_backbone_size = backbone_features.size(1)
            actual_subagent_size = subagent_flat.size(1)
            
            if actual_backbone_size != expected_backbone_size:
                raise ValueError(f"Backbone features size mismatch: expected {expected_backbone_size}, got {actual_backbone_size}")
            
            if actual_subagent_size != expected_subagent_size:
                print(f"Warning: Subagent Q-values size mismatch: expected {expected_subagent_size}, got {actual_subagent_size}")
                # èª¿æ•´å­ä»£ç†Qå€¼å¤§å°
                subagent_flat = self._adjust_subagent_size(subagent_flat, expected_subagent_size)
            
            # ğŸ”§ ä¿®å¾©ï¼šä½¿ç”¨å›ºå®šçš„ ActionBlock
            action_output = self.action_block(backbone_features, subagent_flat)
            
            # ğŸ”§ æ–°å¢ï¼šè¼¸å‡ºæ¨™æº–åŒ–å’Œé™åˆ¶
            action_output = self.output_norm(action_output)
            action_output = torch.tanh(action_output) * self.output_scale  # é™åˆ¶è¼¸å‡ºç¯„åœ
            
            # æœ€çµ‚æª¢æŸ¥
            if torch.isnan(action_output).any() or torch.isinf(action_output).any():
                print("Warning: action_output contains NaN or Inf after normalization")
                return torch.zeros(batch_size, 3, device=market_data.device)
            
            return action_output
            
        except Exception as e:
            print(f"Error in MSCNN forward: {e}")
            batch_size = market_data.size(0) if market_data.dim() > 0 else 1
            device = market_data.device if hasattr(market_data, 'device') else torch.device('cpu')
            return torch.zeros(batch_size, 3, device=device)
    
    def _standardize_inputs(self, market_data, sub_agent_qvalues):
        """æ¨™æº–åŒ–è¼¸å…¥ç¶­åº¦"""
        # è™•ç†market_data
        if market_data.dim() == 2:  # [T, N] -> [1, T, N]
            market_data = market_data.unsqueeze(0)
        elif market_data.dim() == 1:  # [N] -> [1, 1, N]
            market_data = market_data.unsqueeze(0).unsqueeze(0)
        
        # è™•ç†sub_agent_qvalues
        if sub_agent_qvalues.dim() == 2:  # [n_agents, 3] -> [1, n_agents, 3]
            sub_agent_qvalues = sub_agent_qvalues.unsqueeze(0)
        elif sub_agent_qvalues.dim() == 1:  # [features] -> [1, n_agents, 3]
            # å‡è¨­æ˜¯å±•å¹³çš„Qå€¼ï¼Œé‡æ–°reshape
            total_qvalues = sub_agent_qvalues.size(0)
            if total_qvalues % 3 == 0:
                n_agents = total_qvalues // 3
                sub_agent_qvalues = sub_agent_qvalues.view(1, n_agents, 3)
            else:
                # å¦‚æœç„¡æ³•æ•´é™¤ï¼Œä½¿ç”¨é»˜èªå½¢ç‹€
                sub_agent_qvalues = sub_agent_qvalues.unsqueeze(0).unsqueeze(0)
        
        return market_data, sub_agent_qvalues
    
    def _process_subagent_qvalues(self, sub_agent_qvalues, batch_size):
        """è™•ç†å­ä»£ç†Qå€¼"""
        # å±•å¹³å­ä»£ç†Qå€¼: [B, n_agents, 3] -> [B, n_agents * 3]
        subagent_flat = sub_agent_qvalues.reshape(batch_size, -1)
        return subagent_flat
    
    def _adjust_subagent_size(self, subagent_flat, expected_size):
        """èª¿æ•´å­ä»£ç†Qå€¼å¤§å°ä»¥åŒ¹é…æœŸæœ›å°ºå¯¸"""
        current_size = subagent_flat.size(1)
        
        if current_size < expected_size:
            # å¦‚æœç•¶å‰å°ºå¯¸å°æ–¼æœŸæœ›ï¼Œç”¨é›¶å¡«å……
            padding = torch.zeros(subagent_flat.size(0), expected_size - current_size, 
                                device=subagent_flat.device, dtype=subagent_flat.dtype)
            subagent_flat = torch.cat([subagent_flat, padding], dim=1)
        elif current_size > expected_size:
            # å¦‚æœç•¶å‰å°ºå¯¸å¤§æ–¼æœŸæœ›ï¼Œæˆªå–å‰é¢éƒ¨åˆ†
            subagent_flat = subagent_flat[:, :expected_size]
        
        return subagent_flat

    def get_model_info(self):
        """ç²å–æ¨¡å‹ä¿¡æ¯ç”¨æ–¼èª¿è©¦"""
        return {
            'backbone_output_channels': self.backbone_output_channels,
            'action_block_exists': self.action_block is not None,
            'action_block_backbone_size': self.action_block.backbone_features_size if self.action_block else None,
            'action_block_subagent_size': self.action_block.subagent_qvalues_size if self.action_block else None,
            'parameters': list(self.state_dict().keys())
        }
# import math
# class ECABlock(nn.Module):
#     def __init__(self, channels, gamma=2, b=1):
#         super(ECABlock, self).__init__()
#         # Adaptive kernel size as per ECA-Net: k = |log2(C)/Î³ + b/Î³|_odd
#         kernel_size = int(abs(math.log2(channels) / gamma + b / gamma))
#         kernel_size = kernel_size if kernel_size % 2 else kernel_size + 1  # Ensure odd kernel size
#         self.avg_pool = nn.AdaptiveAvgPool2d(1)  # Global average pooling
#         self.conv = nn.Conv1d(
#             in_channels=1,
#             out_channels=1,
#             kernel_size=kernel_size,
#             padding=kernel_size // 2,
#             bias=False
#         )
#         self.sigmoid = nn.Sigmoid()

#     def forward(self, x):
#         # x: (batch_size, channels, height, width)
#         batch_size, channels, _, _ = x.size()
#         # Global average pooling: (batch_size, channels, 1, 1)
#         y = self.avg_pool(x)
#         # Reshape for 1D convolution: (batch_size, 1, channels)
#         y = y.squeeze(-1).transpose(-1, -2)
#         # 1D convolution across channels
#         y = self.conv(y)
#         # Reshape back and apply sigmoid: (batch_size, channels, 1, 1)
#         y = y.transpose(-1, -2).unsqueeze(-1)
#         y = self.sigmoid(y)
#         # Scale input features
#         return x * y

# class MultiScaleCNN(nn.Module):
#     def __init__(self, input_channels=5, sequence_length=10, num_actions=3, configs=None):
#         super(MultiScaleCNN, self).__init__()
        
#         # å¦‚æœæä¾›äº†é…ç½®ï¼Œä½¿ç”¨é…ç½®ä¸­çš„åƒæ•¸ï¼Œå¦å‰‡ä½¿ç”¨é»˜èªå€¼
#         self.input_channels = input_channels
#         self.sequence_length = sequence_length
#         self.num_actions = num_actions

#         # Multi-Scale Feature Extraction Modules
#         # è¼¸å…¥å½¢ç‹€: (batch_size, sequence_length=10, input_channels=5)
#         # Single-scale (1D convolution along sequence dimension)
#         self.conv_single = nn.Conv1d(
#             in_channels=self.input_channels,  # 5 features
#             out_channels=5,  # As per "stack into five-dimensional features"
#             kernel_size=3,
#             padding=1
#         )

#         # Medium-scale (2D convolution, 3x3 kernel)
#         self.conv_medium = nn.Conv2d(
#             in_channels=1,
#             out_channels=2,  # To contribute to 8 total channels
#             kernel_size=(3, 3),
#             padding=(1, 1)
#         )

#         # Global-scale (2D convolution, 5x5 kernel)
#         self.conv_global = nn.Conv2d(
#             in_channels=1,
#             out_channels=1,  # To contribute to 8 total channels
#             kernel_size=(5, 5),
#             padding=(2, 2)
#         )

#         # Backbone CNN with conditional Batch Normalization
#         self.backbone_conv = nn.Conv2d(
#             in_channels=8,  # 8 channels from concatenated multi-scale output
#             out_channels=16,
#             kernel_size=(3, 3),
#             padding=(1, 1)
#         )
        
#         # æ·»åŠ  Batch Normalization
#         self.backbone_bn = nn.BatchNorm2d(16)
        
#         # ECA Block after backbone convolution
#         self.eca = ECABlock(channels=16)
#         self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)

#         # Fully Connected Layers
#         # Calculate flattened size after pooling - dynamic calculation
#         # After pooling, spatial dimensions are halved
#         pooled_height = self.sequence_length // 2 if self.sequence_length % 2 == 0 else (self.sequence_length + 1) // 2
#         pooled_width = self.input_channels // 2 if self.input_channels % 2 == 0 else (self.input_channels + 1) // 2
#         self.fc_input_size = 16 * pooled_height * pooled_width
#         self.fc1 = nn.Linear(self.fc_input_size, 128)
#         # æ·»åŠ  Batch Normalization for FC layers
#         self.fc1_bn = nn.BatchNorm1d(128)
#         self.fc2 = nn.Linear(128, self.num_actions)

#     def forward(self, market_data, sub_agent_qvalues):
#         """
#         å‰å‘å‚³æ’­
        
#         :param market_data: å¸‚å ´æ•¸æ“š (batch_size, window_size, 5) - OHLCVæ•¸æ“š
#         :param sub_agent_qvalues: å­ä»£ç†Qå€¼ (batch_size, n_agents, 3) æˆ– (n_agents, 3)
#         :return: å‹•ä½œQå€¼ (batch_size, 3)
#         """
#         # è™•ç†å¸‚å ´æ•¸æ“šçš„è¼¸å…¥å½¢ç‹€
#         if market_data.dim() == 2:
#             # å¦‚æœè¼¸å…¥æ˜¯ (sequence_length, input_channels)ï¼Œæ·»åŠ  batch ç¶­åº¦
#             market_data = market_data.unsqueeze(0)  # (1, sequence_length, input_channels)
#         elif market_data.dim() == 3:
#             # å·²ç¶“æ˜¯æ­£ç¢ºçš„ 3D å½¢ç‹€: (batch_size, sequence_length, input_channels)
#             pass
#         else:
#             raise ValueError(f"Expected 2D or 3D market_data input, got {market_data.dim()}D with shape {market_data.shape}")
        
#         # è™•ç†å­ä»£ç†Qå€¼çš„è¼¸å…¥å½¢ç‹€
#         if sub_agent_qvalues.dim() == 2:
#             # å¦‚æœè¼¸å…¥æ˜¯ (n_agents, 3)ï¼Œæ·»åŠ  batch ç¶­åº¦
#             sub_agent_qvalues = sub_agent_qvalues.unsqueeze(0)  # (1, n_agents, 3)
#         elif sub_agent_qvalues.dim() == 3:
#             # å·²ç¶“æ˜¯æ­£ç¢ºçš„ 3D å½¢ç‹€: (batch_size, n_agents, 3)
#             pass
#         else:
#             raise ValueError(f"Expected 2D or 3D sub_agent_qvalues input, got {sub_agent_qvalues.dim()}D with shape {sub_agent_qvalues.shape}")
        
#         # è¼¸å…¥å½¢ç‹€: (batch_size, sequence_length, input_channels)
#         batch_size, sequence_length, input_channels = market_data.size()
        
#         # è½‰ç½®ç‚ºå·ç©å±¤æœŸæœ›çš„æ ¼å¼: (batch_size, input_channels, sequence_length)
#         market_data = market_data.transpose(1, 2)  # (batch_size, input_channels, sequence_length)

#         # Process daily data with shared weights
#         def process_scale(data):
#             # Single-scale: 1D convolution
#             single_features = F.relu(self.conv_single(data))  # (batch_size, 5, 10)

#             # Reshape for 2D convolutions: treat as single-channel 2D image
#             # Shape: (batch_size, 1, 5, 10)
#             single_features_2d = single_features.unsqueeze(1)

#             # Medium-scale: 2D convolution
#             medium_features = F.relu(self.conv_medium(single_features_2d))  # (batch_size, 2, 5, 10)

#             # Global-scale: 2D convolution
#             global_features = F.relu(self.conv_global(single_features_2d))  # (batch_size, 1, 5, 10)

#             # å°‡1Då·ç©çµæœé‡å¡‘ç‚º2Dä»¥ä¾¿é€£æ¥: (batch_size, 5, 10) -> (batch_size, 5, 5, 10)
#             single_features_reshaped = single_features.unsqueeze(2).expand(-1, -1, 5, -1)  # (batch_size, 5, 5, 10)

#             # Concatenate to form 8-channel output: 5 (single) + 2 (medium) + 1 (global) = 8
#             multi_scale_features = torch.cat(
#                 (single_features_reshaped, medium_features, global_features),
#                 dim=1
#             )  # (batch_size, 8, 5, 10)
#             return multi_scale_features

#         # Process market data through multi-scale feature extraction
#         market_features = process_scale(market_data)

#         # Backbone CNN with conditional Batch Normalization
#         x = self.backbone_conv(market_features)
        
#         # åªæœ‰åœ¨ batch size > 1 ä¸”æ¨¡å‹åœ¨è¨“ç·´æ¨¡å¼æ™‚æ‰ä½¿ç”¨ Batch Normalization
#         if batch_size > 1 and self.training:
#             x = self.backbone_bn(x)
        
#         x = F.relu(x)
        
#         # Apply ECA block
#         x = self.eca(x)
#         x = self.avg_pool(x)  # (batch_size, 16, pooled_height, pooled_width)

#         # Flatten market features
#         market_flat = x.view(batch_size, -1)
        
#         # Flatten sub-agent Q-values
#         sub_qvalues_flat = sub_agent_qvalues.view(batch_size, -1)  # (batch_size, n_agents * 3)
        
#         # Concatenate market features and sub-agent Q-values
#         combined_features = torch.cat([market_flat, sub_qvalues_flat], dim=1)
        
#         # å‹•æ…‹è¨ˆç®—æ–°çš„FCå±¤è¼¸å…¥å°ºå¯¸
#         combined_input_size = combined_features.size(1)
        
#         # å¦‚æœFCå±¤å°ºå¯¸ä¸åŒ¹é…ï¼Œé‡æ–°åˆå§‹åŒ–
#         if not hasattr(self, 'fc1_combined') or self.fc1_combined.in_features != combined_input_size:
#             self.fc1_combined = nn.Linear(combined_input_size, 128).to(combined_features.device)
#             self.fc1_combined_bn = nn.BatchNorm1d(128).to(combined_features.device)
#             self.fc2_combined = nn.Linear(128, self.num_actions).to(combined_features.device)
#             # æ·»åŠ  Dropout å±¤
#             self.dropout = nn.Dropout(0.1).to(combined_features.device)
        
#         # Fully Connected Layers with conditional Batch Normalization
#         x = self.fc1_combined(combined_features)
        
#         # åªæœ‰åœ¨ batch size > 1 ä¸”æ¨¡å‹åœ¨è¨“ç·´æ¨¡å¼æ™‚æ‰ä½¿ç”¨ Batch Normalization
#         if batch_size > 1 and self.training:
#             x = self.fc1_combined_bn(x)  # æ·»åŠ  Batch Normalization
        
#         x = F.tanh(x)  # Tanh activation as per paper
        
#         # æ·»åŠ  Dropout (åªåœ¨è¨“ç·´æ¨¡å¼ä¸‹ä½¿ç”¨)
#         if self.training:
#             x = self.dropout(x)
        
#         x = self.fc2_combined(x)  # (batch_size, 3)
#         return x